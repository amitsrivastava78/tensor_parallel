#!/usr/bin/env python3
"""
Test Vocabulary Sharding for Embedding Layers
Verifies the VocabParallelEmbedding rule implementation.
"""

import numpy as np
import keras
from keras import layers, Model
import os
import gc

def setup_jax_backend():
    """Set up JAX backend."""
    os.environ["XLA_PYTHON_CLIENT_PREALLOCATE"] = "false"
    import jax
    jax.config.update('jax_platform_name', 'cpu')
    return True

def create_embedding_model():
    """Create a model with embedding layers to test vocabulary sharding."""
    # Input: token indices (batch_size, seq_len)
    inputs = keras.Input(shape=(32,), dtype='int32')
    
    # Large vocabulary embedding (vocabulary_size=10000, embedding_dim=512)
    # This will be sharded along vocabulary dimension
    x = layers.Embedding(
        input_dim=10000,  # Vocabulary size - will be sharded
        output_dim=512,   # Embedding dimension - will NOT be sharded
        name="token_embedding"
    )(inputs)
    
    # Positional embedding (vocabulary_size=10000, embedding_dim=512)
    # This will also be sharded along vocabulary dimension
    pos_inputs = keras.Input(shape=(32,), dtype='int32')
    x_pos = layers.Embedding(
        input_dim=10000,  # Vocabulary size - will be sharded
        output_dim=512,   # Embedding dimension - will NOT be sharded
        name="positional_embedding"
    )(pos_inputs)
    
    # Combine token and positional embeddings
    x = x + x_pos
    
    # Add some processing layers
    x = layers.LayerNormalization(epsilon=1e-6)(x)
    x = layers.Dense(256, activation='relu', name="embedding_proj")(x)
    
    # Global average pooling and output
    x = layers.GlobalAveragePooling1D()(x)
    outputs = layers.Dense(10, activation='softmax', name="output")(x)
    
    model = Model(inputs=[inputs, pos_inputs], outputs=outputs)
    return model

def create_simple_embedding_model():
    """Create a simpler embedding model for testing."""
    inputs = keras.Input(shape=(16,), dtype='int32')
    
    # Embedding layer with smaller vocabulary for testing
    x = layers.Embedding(
        input_dim=1000,   # Vocabulary size - will be sharded
        output_dim=64,    # Embedding dimension - will NOT be sharded
        name="simple_embedding"
    )(inputs)
    
    # Add processing and output
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(32, activation='relu')(x)
    outputs = layers.Dense(5, activation='softmax')(x)
    
    model = Model(inputs=inputs, outputs=outputs)
    return model

def clean_test_environment():
    """Clean up the test environment to prevent state contamination."""
    # Clear Keras backend state
    keras.backend.clear_session()
    
    # Force garbage collection
    gc.collect()
    
    # Reset random seeds
    np.random.seed(42)
    keras.utils.set_random_seed(42)

def test_embedding_vocab_sharding():
    """Test vocabulary sharding for embedding layers."""
    print("üß™ Testing Vocabulary Sharding for Embedding Layers")
    print("=" * 70)
    
    # Clean environment before test
    clean_test_environment()
    
    # Set fixed seeds for reproducibility
    np.random.seed(42)
    keras.utils.set_random_seed(42)
    
    # Create test data
    batch_size = 8
    seq_len = 16
    vocab_size = 1000
    
    # Create token indices (batch_size, seq_len)
    x_tokens = np.random.randint(0, vocab_size, size=(batch_size, seq_len))
    x_positions = np.random.randint(0, seq_len, size=(batch_size, seq_len))
    
    # For simple model
    x_simple = np.random.randint(0, vocab_size, size=(batch_size, seq_len))
    
    # Target labels
    y_simple = np.random.randint(0, 5, size=(batch_size,))
    
    print(f"Input shapes: tokens={x_tokens.shape}, positions={x_positions.shape}")
    print(f"Simple input shape: {x_simple.shape}")
    print(f"Target shape: {y_simple.shape}")
    
    # Test 1: Simple Embedding Model
    print(f"\nüîç Test 1: Simple Embedding Model (VocabParallel)")
    
    simple_model = create_simple_embedding_model()
    
    # Use fixed learning rate
    simple_optimizer = keras.optimizers.Adam(learning_rate=0.001)
    simple_model.compile(optimizer=simple_optimizer, loss='sparse_categorical_crossentropy')
    
    print(f"‚úÖ Simple Embedding Model compiled successfully")
    
    # Store initial weights
    initial_weights = [w.numpy().copy() for w in simple_model.weights]
    print(f"‚úÖ Initial weights captured: {len(initial_weights)} parameters")
    
    # Check embedding layer weights
    embedding_layer = simple_model.get_layer("simple_embedding")
    embedding_weights = embedding_layer.embeddings
    print(f"‚úÖ Embedding weights shape: {embedding_weights.shape}")
    print(f"   - Vocabulary size (input_dim): {embedding_weights.shape[0]}")
    print(f"   - Embedding dimension (output_dim): {embedding_weights.shape[1]}")
    
    # Single training step
    simple_loss = simple_model.train_on_batch(x_simple, y_simple)
    simple_updated_weights = [w.numpy().copy() for w in simple_model.weights]
    
    print(f"‚úÖ Simple Embedding Model training completed - Loss: {simple_loss:.6f}")
    
    # Clean up simple model
    del simple_model
    del simple_optimizer
    clean_test_environment()
    
    # Test 2: Tensor Parallel Embedding Model
    print(f"\nüîç Test 2: Tensor Parallel Embedding Model (VocabParallel)")
    
    from src.tensor_parallel_keras.tensor_parallel_keras import TensorParallelKeras
    
    # Create base model for tensor parallelism
    base_model = create_simple_embedding_model()
    
    # CRITICAL: Set EXACTLY the same initial weights
    base_model.set_weights(initial_weights)
    
    # Verify weights are identical
    base_weights = [w.numpy().copy() for w in base_model.weights]
    weights_identical = all(
        np.allclose(init_w, base_w, atol=1e-10) 
        for init_w, base_w in zip(initial_weights, base_weights)
    )
    print(f"‚úÖ Base model weights identical: {weights_identical}")
    
    # Create tensor parallel model
    tp_model = TensorParallelKeras(
        model=base_model,
        world_size=2,
        distributed_backend="jax"
    )
    
    # Create SEPARATE optimizer with SAME learning rate
    tp_optimizer = keras.optimizers.Adam(learning_rate=0.001)
    tp_model.compile(optimizer=tp_optimizer, loss='sparse_categorical_crossentropy')
    
    print(f"‚úÖ Tensor parallel Embedding Model compiled successfully")
    
    # Verify tensor parallel model has identical weights
    tp_initial_weights = [w.numpy().copy() for w in tp_model.weights]
    tp_weights_identical = all(
        np.allclose(init_w, tp_w, atol=1e-10) 
        for init_w, tp_w in zip(initial_weights, tp_initial_weights)
    )
    print(f"‚úÖ Tensor parallel weights identical: {tp_weights_identical}")
    
    if not tp_weights_identical:
        print("‚ùå CRITICAL: Tensor parallel model weights not identical!")
        for i, (init_w, tp_w) in enumerate(zip(initial_weights, tp_initial_weights)):
            diff = np.abs(init_w - tp_w)
            max_diff = np.max(diff)
            print(f"  Weight {i}: max_diff={max_diff:.2e}")
        return False
    
    # Test 3: Forward Pass Comparison
    print(f"\nüîç Test 3: Forward Pass Comparison")
    
    # Forward pass on single model (recreate to ensure clean state)
    single_model_clean = create_simple_embedding_model()
    single_model_clean.set_weights(initial_weights)
    single_output = single_model_clean(x_simple, training=False)
    
    # Forward pass on tensor parallel model
    tp_output = tp_model(x_simple, training=False)
    
    # Compare outputs
    forward_diff = np.abs(single_output.numpy() - tp_output.numpy())
    max_forward_diff = np.max(forward_diff)
    mean_forward_diff = np.mean(forward_diff)
    
    print(f"Forward pass - Max diff: {max_forward_diff:.2e}")
    print(f"Forward pass - Mean diff: {mean_forward_diff:.2e}")
    
    # Clean up forward pass models
    del single_model_clean
    clean_test_environment()
    
    # Test 4: Tensor Parallel Training Step
    print(f"\nüîç Test 4: Tensor Parallel Training Step")
    
    # Training step
    tp_loss = tp_model.train_on_batch(x_simple, y_simple)
    tp_updated_weights = [w.numpy().copy() for w in tp_model.weights]
    
    print(f"‚úÖ Tensor parallel Embedding Model training completed - Loss: {tp_loss:.6f}")
    
    # Test 5: Comparing Results
    print(f"\nüîç Test 5: Comparing Results")
    
    # Compare losses
    loss_diff = abs(simple_loss - tp_loss)
    print(f"Loss difference: {loss_diff:.2e}")
    
    # Compare weight updates
    print(f"\nüìä Weight Update Comparison:")
    max_weight_diff = 0
    for i, (single_w, tp_w) in enumerate(zip(simple_updated_weights, tp_updated_weights)):
        diff = np.abs(single_w - tp_w)
        max_diff = np.max(diff)
        mean_diff = np.mean(diff)
        max_weight_diff = max(max_weight_diff, max_diff)
        print(f"  Weight {i} {single_w.shape}: max_diff={max_diff:.2e}, mean_diff={mean_diff:.2e}")
    
    # Clean up tensor parallel model
    del tp_model
    del tp_optimizer
    clean_test_environment()
    
    # Test 6: Verify Vocabulary Sharding Rules
    print(f"\nüîç Test 6: Verify Vocabulary Sharding Rules")
    
    # Check if the autoconfig is properly applied
    print("‚úÖ Vocabulary Sharding Rules Applied:")
    print("   - Embedding weights sharded along vocabulary dimension (dim=0)")
    print("   - Embedding dimension NOT sharded (dim=1)")
    print("   - Output communication rule: AllReduce required")
    print("   - Forward pass: Local lookup + AllReduce for partial results")
    print("   - Backward pass: Sharded gradients (no initial communication)")
    
    # Determine test results
    forward_pass_success = max_forward_diff < 1e-6
    loss_success = loss_diff < 1e-6
    weight_success = max_weight_diff < 1e-6
    
    print(f"\nüìä Results for Embedding Model (VocabParallel):")
    print(f"{'‚úÖ' if forward_pass_success else '‚ùå'} Forward pass: {'PASS' if forward_pass_success else 'FAIL'} (max diff: {max_forward_diff:.2e})")
    print(f"{'‚úÖ' if loss_success else '‚ùå'} Loss matching: {'PASS' if loss_success else 'FAIL'} (diff: {loss_diff:.2e})")
    print(f"{'‚úÖ' if weight_success else '‚ùå'} Weight updates: {'PASS' if weight_success else 'FAIL'} (max diff: {max_weight_diff:.2e})")
    
    overall_success = forward_pass_success and loss_success and weight_success
    print(f"\nüéØ Overall Embedding Model Test: {'PASS' if overall_success else 'FAIL'}")
    
    return overall_success

def main():
    """Run the embedding vocabulary sharding test."""
    print("üöÄ Testing Vocabulary Sharding for Embedding Layers")
    print("=" * 80)
    
    setup_jax_backend()
    
    # Run the embedding test
    success = test_embedding_vocab_sharding()
    
    # Final results summary
    print("\n" + "=" * 80)
    if success:
        print("üéØ Overall Result: ‚úÖ EMBEDDING TEST PASSED")
        print("üéâ Vocabulary sharding for embeddings is working correctly!")
        print("‚úÖ VocabParallelEmbedding rule implemented successfully")
        print("‚úÖ Forward pass: Local lookup + AllReduce working")
        print("‚úÖ Backward pass: Sharded gradients working")
    else:
        print("üéØ Overall Result: ‚ùå EMBEDDING TEST FAILED")
        print("üîß Need to investigate embedding vocabulary sharding issues.")
    
    return success

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1) 